{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Modules\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract features from audio file\n",
    "def getFeaturesTest(filename):\n",
    "\n",
    "    # Reading File\n",
    "    y,sr=librosa.load(filename)\n",
    "\n",
    "    # Mel Frequency Cepstral Coefficients\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
    "\n",
    "    # Short Term Fourier Transform\n",
    "    stft = np.abs(librosa.stft(y))\n",
    "\n",
    "    # Chromagram from STFT\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "\n",
    "    # Mel Spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T,axis=0)\n",
    "\n",
    "    # Spectral Contrasts\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sr).T,axis=0)\n",
    "\n",
    "    # Tonal Centroid Features\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y),sr=sr).T,axis=0)\n",
    "\n",
    "    # Flatten the array\n",
    "    features = np.concatenate((mfcc, chroma, mel, contrast, tonnetz), axis=0)\n",
    "    return np.array([features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model Files and Defining Output Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models\n",
    "cnn = load_model('CNN_Best_Model.hdf5')\n",
    "crnn = load_model('CRNN_Best_Model.hdf5')\n",
    "bicrnn = load_model('BICRNN_Best_Model.hdf5')\n",
    "resnet = load_model('RESNET_Best_Model.hdf5')\n",
    "\n",
    "# Defining output labels\n",
    "outputLabels = {\n",
    "\t0 : 'Axecutting Sound',\n",
    "\t1 : 'Chainsaw Sound',\n",
    "\t2 : 'Forest Sound',\n",
    "\t3 : 'Handsaw Sound',\n",
    "\t4 : 'Rain / Thunder Sound',\n",
    "\t5 : 'Wind Sound'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Audio Files and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio1 = getFeaturesTest('Axecutting_test1.wav')\n",
    "audio2 = getFeaturesTest('Rain&Thunder_test1.wav')\n",
    "audio3 = getFeaturesTest('Wind_test1.wav')\n",
    "\n",
    "y1 = 0\n",
    "y2 = 4\n",
    "y3 = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using models and comparing outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File 1:\n",
      "Labelled Output : Axecutting Sound\n",
      "Predicted Output : Axecutting Sound\n",
      "\n",
      "Audio File 2:\n",
      "Labelled Output : Rain / Thunder Sound\n",
      "Predicted Output : Rain / Thunder Sound\n",
      "\n",
      "Audio File 3:\n",
      "Labelled Output : Wind Sound\n",
      "Predicted Output : Wind Sound\n"
     ]
    }
   ],
   "source": [
    "pred1 = np.argmax(cnn.predict(audio1, verbose=0), axis=1)\n",
    "pred2 = np.argmax(cnn.predict(audio2, verbose=0), axis=1)\n",
    "pred3 = np.argmax(cnn.predict(audio3, verbose=0), axis=1)\n",
    "\n",
    "print('Audio File 1:')\n",
    "print('Labelled Output :', outputLabels[y1])\n",
    "print('Predicted Output :', outputLabels[pred1[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 2:')\n",
    "print('Labelled Output :', outputLabels[y2])\n",
    "print('Predicted Output :', outputLabels[pred2[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 3:')\n",
    "print('Labelled Output :', outputLabels[y3])\n",
    "print('Predicted Output :', outputLabels[pred3[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File 1:\n",
      "Labelled Output : Axecutting Sound\n",
      "Predicted Output : Axecutting Sound\n",
      "\n",
      "Audio File 2:\n",
      "Labelled Output : Rain / Thunder Sound\n",
      "Predicted Output : Rain / Thunder Sound\n",
      "\n",
      "Audio File 3:\n",
      "Labelled Output : Wind Sound\n",
      "Predicted Output : Forest Sound\n"
     ]
    }
   ],
   "source": [
    "pred1 = np.argmax(crnn.predict(audio1, verbose=0), axis=1)\n",
    "pred2 = np.argmax(crnn.predict(audio2, verbose=0), axis=1)\n",
    "pred3 = np.argmax(crnn.predict(audio3, verbose=0), axis=1)\n",
    "\n",
    "print('Audio File 1:')\n",
    "print('Labelled Output :', outputLabels[y1])\n",
    "print('Predicted Output :', outputLabels[pred1[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 2:')\n",
    "print('Labelled Output :', outputLabels[y2])\n",
    "print('Predicted Output :', outputLabels[pred2[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 3:')\n",
    "print('Labelled Output :', outputLabels[y3])\n",
    "print('Predicted Output :', outputLabels[pred3[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BiCRNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File 1:\n",
      "Labelled Output : Axecutting Sound\n",
      "Predicted Output : Axecutting Sound\n",
      "\n",
      "Audio File 2:\n",
      "Labelled Output : Rain / Thunder Sound\n",
      "Predicted Output : Rain / Thunder Sound\n",
      "\n",
      "Audio File 3:\n",
      "Labelled Output : Wind Sound\n",
      "Predicted Output : Wind Sound\n"
     ]
    }
   ],
   "source": [
    "pred1 = np.argmax(bicrnn.predict(audio1, verbose=0), axis=1)\n",
    "pred2 = np.argmax(bicrnn.predict(audio2, verbose=0), axis=1)\n",
    "pred3 = np.argmax(bicrnn.predict(audio3, verbose=0), axis=1)\n",
    "\n",
    "print('Audio File 1:')\n",
    "print('Labelled Output :', outputLabels[y1])\n",
    "print('Predicted Output :', outputLabels[pred1[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 2:')\n",
    "print('Labelled Output :', outputLabels[y2])\n",
    "print('Predicted Output :', outputLabels[pred2[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 3:')\n",
    "print('Labelled Output :', outputLabels[y3])\n",
    "print('Predicted Output :', outputLabels[pred3[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio File 1:\n",
      "Labelled Output : Axecutting Sound\n",
      "Predicted Output : Axecutting Sound\n",
      "\n",
      "Audio File 2:\n",
      "Labelled Output : Rain / Thunder Sound\n",
      "Predicted Output : Rain / Thunder Sound\n",
      "\n",
      "Audio File 3:\n",
      "Labelled Output : Wind Sound\n",
      "Predicted Output : Axecutting Sound\n"
     ]
    }
   ],
   "source": [
    "pred1 = np.argmax(resnet.predict(audio1, verbose=0), axis=1)\n",
    "pred2 = np.argmax(resnet.predict(audio2, verbose=0), axis=1)\n",
    "pred3 = np.argmax(resnet.predict(audio3, verbose=0), axis=1)\n",
    "\n",
    "print('Audio File 1:')\n",
    "print('Labelled Output :', outputLabels[y1])\n",
    "print('Predicted Output :', outputLabels[pred1[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 2:')\n",
    "print('Labelled Output :', outputLabels[y2])\n",
    "print('Predicted Output :', outputLabels[pred2[0]], end='\\n\\n')\n",
    "\n",
    "print('Audio File 3:')\n",
    "print('Labelled Output :', outputLabels[y3])\n",
    "print('Predicted Output :', outputLabels[pred3[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
